print("""
      ⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⣀⣼⣄⢻⣆⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣿⣷⣿⣿⣿⣿⢹⡗⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣿⣿⣇⠈⢷⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣿⣿⡿⡆⠀⠈⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣾⣿⣦⠀⡀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣏⣿⣿⣽⡀⠀⠘⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣸⣿⣿⣿⣿⣿⣄
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⣿⣿⣿⣿⣿⣿⣇⢲⡀⠈⢻⡦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⠇⣸⣿⣿⣿⣿⣿⣿
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣿⠷⢻⣿⣌⣿⡆⢸⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣴⡏⣵⣿⣿⣿⣿⣿⣿⣿
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢿⣿⣿⣿⣧⡼⣹⡿⢸⣧⠀⠙⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣼⣾⡟⠉⣭⣿⣿⣭⣿⣿⣿⡇
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⣿⣿⣿⣧⡟⣠⡼⢿⠆⠀⠘⣷⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣗⣲⣦⣤⣤⣀⡀⠀⠀⠀⠀⢀⣠⣿⢹⠹⠁⢲⣿⣿⣿⣾⣿⣿⡿⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⣴⣾⣿⣿⣿⣿⣼⣿⣧⣾⣷⠒⠶⢿⣭⣿⠿⠿⢶⣾⡻⠋⠾⠟⠙⠿⠗⠒⣻⣿⣿⣶⣶⡶⣾⣿⡿⡾⠈⣰⣴⡿⣿⣷⣿⣿⣿⡿⠁⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣤⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣻⡟⠛⢒⣤⡄⠉⠽⠶⠤⠄⠈⢉⣀⣐⣚⡛⠓⠒⠛⠻⠀⠀⢀⣿⣦⣭⡞⣤⣶⣾⣿⣿⠻⣦⣿⣿⣿⠟⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣿⡿⢛⣿⡿⢟⣿⡿⢿⣿⣿⣿⣿⠿⢰⢾⣿⣿⣾⠧⠤⢄⠴⠾⠿⣥⡤⠤⠀⢠⣄⣀⣀⠘⣷⡀⢣⣍⣉⠀⠀⠘⠻⣿⣄⣠⡟⢿⣿⣿⠏⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⣠⣿⣿⣿⣶⣿⢏⣴⡾⠛⠑⣿⣟⣑⠿⣿⣶⣾⢷⡻⣏⠀⠀⠶⠛⢶⣶⠶⠶⣤⣀⠀⢞⣉⠈⠛⠒⠿⠷⠾⣶⣧⣀⠀⠀⢀⣬⣥⢙⣿⣿⣿⠁⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⢀⣴⣿⢟⣡⣿⣿⠟⣿⠻⠋⠀⣠⣼⡿⢿⣯⣀⠙⢿⣷⡶⣿⣿⣦⠀⠀⠀⠁⢀⠾⠋⠙⠛⠉⠹⠤⠶⣿⣷⣦⡠⢬⣛⡿⢳⠖⠋⠁⢹⣾⣿⣿⣿⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⢠⣾⣿⣿⣿⣿⠟⣿⣾⡿⢠⣤⣾⡿⠛⠦⣤⣀⠙⢷⣾⣿⣾⣾⣿⣿⣧⣤⡀⠀⠀⠀⠀⢸⠃⠀⠀⠀⠀⠀⠚⠋⠀⠴⣏⠳⡄⠀⣀⣤⠼⣛⡿⣿⣿⡄⠀⠀⠀⠀⠀
⠀⠀⠀⢠⡾⢿⠉⣿⣿⠏⡰⠿⠋⠀⢠⣿⣟⠛⣲⣶⡶⠼⣉⠙⣶⡯⠋⠀⢀⣀⣈⡉⠛⢷⣦⣀⠀⠘⣧⡀⠀⠀⠀⠀⠐⠮⠁⠀⠉⢀⣰⡾⠷⣶⣿⡟⢛⣡⢸⡇⠀⠀⠀⠀⠀
⠀⠀⢠⣿⣷⣾⣿⣿⣿⡼⠁⡶⠀⠀⣼⡿⠃⠒⠒⠒⠂⣠⣬⣿⡏⠀⠀⢠⣿⣭⣽⣿⡆⠀⢹⣿⣿⣷⠳⣵⠀⠀⠀⠀⢀⡀⠀⣀⣠⣾⢿⣦⠀⠹⣿⣗⣉⣿⢸⡇⠀⠀⠀⠀⠀
⠀⠀⣼⣿⡟⡽⡟⣠⣿⢧⡜⠁⢨⣿⣿⣛⣀⣀⠀⢀⣸⡿⢚⣿⡇⠀⠀⠈⣿⣿⣿⣿⡏⠀⠀⢿⣿⣿⠀⠈⠛⢦⡴⠀⠘⠛⠋⠁⣼⣷⣾⣿⠀⠀⢿⡛⠒⠛⠈⡇⠀⠀⠀⠀⠀
⠀⣰⣿⢿⣹⡇⢸⡏⣿⠸⠁⠀⠀⣿⣟⣓⠶⠦⠀⢀⣀⣀⢽⣞⣻⣆⠀⠀⠈⠙⠛⠋⠀⣀⣤⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⣿⠿⠟⠀⠀⣾⢍⣻⡂⠀⡇⠀⠀⠀⠀⠀
⠀⣿⣿⣸⣿⣧⣼⠀⣿⠀⠀⠀⠀⢿⣿⣧⡴⢖⣚⣭⠽⢷⣾⣯⡭⠙⠻⣶⣤⣀⣤⣶⠾⣋⣿⣿⠟⠁⠀⠲⠦⣴⣶⣦⠀⠀⠀⠀⠘⣇⡀⠀⣀⣼⢯⣄⡉⢉⣤⡇⠀⠀⠀⠀⠀
⢠⣿⣿⡏⢸⡟⣿⠀⠀⠀⠀⠀⢀⠈⠻⣿⣶⡟⠉⣯⣤⣾⣻⡥⢶⣾⡼⠁⣀⣭⣤⣾⡿⠟⠋⠀⠀⠀⠀⠀⠀⢰⣿⣿⣇⠀⠀⠀⠀⠘⠛⢿⣿⠀⣩⡟⠧⡿⢸⡇⠀⠀⠀⠀⠀
⢸⣿⣿⣿⡟⠀⠿⠀⠀⠀⠀⠀⠻⣷⣶⣾⣿⣡⣤⡞⠉⠛⠛⠟⣡⡴⠿⠛⠟⠛⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣿⣷⣿⡆⠀⠀⠀⠀⠀⠀⠹⣿⣯⣙⠲⠖⣼⡇⠀⠀⠀⠀⠀
⢸⣿⣶⢿⡇⠀⠀⠀⠀⠀⠀⠀⠀⣼⣿⣿⣿⣿⣿⣡⣾⣟⣶⠿⢧⣠⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣆⣀⣮⣿⣿⣹⣿⣇⠀⢠⠀⠀⠀⠰⣤⣌⢻⣌⡿⣶⣿⠇⠀⠀⠀⠀⠀
⢸⠛⣯⠋⠟⠀⠀⠀⠀⠀⠀⠀⠀⠘⢫⣿⠛⢻⣿⣯⣴⣿⢚⠋⠁⠰⠶⠿⡶⡇⠀⠀⠀⢠⡄⢠⡄⣷⣿⣹⣿⣿⣿⣇⠘⢿⠀⠈⠃⢀⡀⠘⢮⣿⣳⣿⣇⣿⡿⠀⠀⠀⠀⠀⠀
⣾⠀⠈⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠁⠀⠋⠉⠙⠛⠋⠸⣷⣬⢷⡦⠀⢀⣀⠀⠀⠀⠛⠿⣷⣿⣿⣿⣿⣿⣿⣿⣿⡆⢸⠃⠀⠀⠈⠷⠳⠀⠙⣃⣼⣿⣿⣧⡀⠀⠀⠀⠀⠀
⢿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣰⣞⠿⣶⡘⢿⣷⣦⣲⣄⠀⠈⠙⠾⣿⢿⣿⡿⣿⣿⢡⡏⠀⠀⠀⠀⠀⠀⠀⣸⢯⣍⠙⠻⢿⡇⠀⠀⠀⠀⠀
⠈⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⠘⠛⠋⢳⣤⡙⢷⡾⣿⣧⣄⠀⠀⠈⢣⡙⠟⠛⣿⡿⠁⠀⠀⠀⠀⠀⠀⣠⣟⡛⠁⠀⠀⢸⣧⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠤⠤⣄⣀⠀⠀⠀⠀⠀⠀⠀⠘⠧⣻⣌⠻⣌⢃⣀⠀⢀⠙⠂⠀⠋⠀⢀⡠⠞⢿⣓⣿⡿⢿⡿⠛⠃⠤⢤⣠⣇⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣸⣿⣿⣦⡀⠀⠀⠀⠀⠀⠀⠙⠿⠦⠹⣷⣍⡳⣞⠷⡄⠀⢠⣤⣾⣶⣷⢿⣿⣟⣻⣶⣤⡀⠀⠲⣄⡈⢿⣷⡀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣟⠳⣭⡙⠓⠤⠀⠀⠀⢦⣀⠀⠀⠀⠀⠈⠛⠙⠻⢷⡯⣦⡉⠙⢿⣿⠿⣷⠮⠽⠿⠿⣿⠻⢦⣄⣈⡙⢻⣿⣧⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣙⣛⣿⡇⠀⠀⠀⠀⠀⡀⠙⢷⣤⣦⡀⠀⠀⠀⠀⠀⠙⢮⣿⣿⣆⡙⢷⣀⡀⠀⠀⠀⢈⡀⣤⡈⣧⡙⢳⡜⣿⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠛⠋⠁⠀⠀⠀⠀⠀⠙⣦⣈⡉⣾⣿⣷⣶⠀⡀⠀⢶⡶⢿⣿⣯⠁⠀⢧⡙⢲⡤⣤⣈⢻⣿⣿⣿⠁⠈⠑⢸⣆⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢶⣷⣤⣿⣿⣿⡿⠛⠁⢠⡇⠲⣾⣷⣿⣿⣷⣀⠀⣦⠙⢦⠙⠙⠫⢿⣿⣿⣿⣧⣤⣈⣉⣿⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⣿⣿⡿⠿⢧⠀⠀⢸⣅⡓⣶⣽⣿⣿⡾⣘⣦⠈⣿⣿⣧⠄⠀⠀⢹⡿⢿⣿⠟⠋⢁⣿⡇⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢓⡶⣦⠀⠙⠿⠞⠀⠀⠀⠈⠹⡿⢿⣿⡾⠿⠟⠙⠈⠉⢀⣤⣶⣤⠾⠉⢴⣟⣁⣤⣬⣉⣙⣷⢀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠡⢤⣽⠃⣘⡁⠀⠀⠀⠈⠉⠉⠀⠀⠀⣠⠟⢉⣍⣉⠉⢙⣿⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠤⣤⣤⠤⢀⣒⠒⠀⠀⠀⠀⠀⠀⠀⠀⠘⠛⠻⡿⠃⣾⣿⣿⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢉⢻⠶⡃⣠⣶⡾⠇⡏⠀)
""")

import argparse
parser = argparse.ArgumentParser()
parser.add_argument('-f', type=str, help='Plant feature: "leaf","flower","fruit"')
parser.add_argument('-b', type=int, help='Batch size')
parser.add_argument('-s', type=float, help='Train/test split')
parser.add_argument('-e', type=int, help='Epochs')
parser.add_argument('-lr', type=float, help='Learning rate')
parser.add_argument('-k', type=int, help='Top K predictions (for testing)')
parser.add_argument('-scratch',type=bool, help='Retrain from beginning?')
args = parser.parse_args()

features = ([args.f] if args.f is not None else ['fruit','leaf','flower'])
batchSize = (args.b if args.b is not None else 16)
trainSplit = (args.s if args.s is not None else 0.75)
numEpochs = (args.e if args.e is not None else 16)
lr = (args.lr if args.lr is not None else 0.0003)
k = (args.k if args.k is not None else 9)
scratch = (args.scratch if args.scratch is not None else False)

from app.plantvision import PlantVision
import random
import torch
import torchvision.transforms as T
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import pickle as pkl
import numpy as np
from torchvision.io import read_image
from tqdm import tqdm

for feature in features:
    print(f'{feature.title()}')
    with open(f'app/resources/{feature}LabelSet.pkl', 'rb') as f:
        labelSet = pkl.load(f)

    with open(f'app/resources/{feature}ImageLabels.pkl', 'rb') as f:
        imageLabels = pkl.load(f)

    with open(f'app/resources/{feature}ImageIndices.pkl','rb') as f:
        imageIndices = pkl.load(f)

    with open(f'app/resources/{feature}MeansAndStds.pkl', 'rb') as f:
        meansAndStds = pkl.load(f)

    preprocess = torch.nn.Sequential( 
    T.CenterCrop(224),
    T.RandomHorizontalFlip(), 
    T.RandomVerticalFlip(), 
    T.RandomAutocontrast(),
    T.ConvertImageDtype(torch.float32),
    T.Normalize(
        mean=meansAndStds['mean'],
        std=meansAndStds['std']
    ))

    def stackImages(index, batchSize, imageIndices):
        start = index * batchSize
        end = (index + 1) * batchSize
        iter = start
        tensors = []
        while iter<end:
            image = read_image(f'app/images/img{imageIndices[iter]}.jpeg').to(device)
            tensors.append(preprocess(image))
            iter += 1
        return torch.stack(tensors)

    def stackLabels(index, batchSize, imageIndices):
        start = index * batchSize
        end = (index + 1) * batchSize
        iter = start
        tensors = []
        while iter<end:
            tensors.append(torch.tensor(imageLabels[imageIndices[iter]]).to(device))
            iter += 1
        return torch.stack(tensors)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    num_classes = len(labelSet) 

    if scratch == True:
        model = PlantVision(num_classes)
        model.to(device)
    else:
        model = PlantVision(num_classes=len(labelSet))
        model.load_state_dict(torch.load(fr"models/plantvision-model-{feature}.pt"), strict=False)
        model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    iterations = np.arange(round(len(imageIndices)/batchSize)-1)

    if scratch==True:
        trainBatches = random.sample(sorted(iterations), k = round(len(iterations)*trainSplit))
        testBatches = [i for i in iterations if i not in trainBatches]
        with open(f'batches/{feature}TrainBatches.pkl','wb') as f:
            pkl.dump(trainBatches, f)
        with open(f'batches/{feature}TestBatches.pkl','wb') as f:
            pkl.dump(testBatches, f)
    else:
        with open(f'batches/{feature}TrainBatches.pkl','rb') as f:
            trainBatches = pkl.load(f)
        with open(f'batches/{feature}TestBatches.pkl', 'rb') as f:
            testBatches = pkl.load(f)
    
    testAccuracies = []
    epoch = 1
    keepGoing = True

    while keepGoing:
        print(f'\nEpoch {epoch}:')
        model.train()
        losses = []
        for i in tqdm(trainBatches):
            images = stackImages(i,batchSize,imageIndices).to(device)
            labels = stackLabels(i,batchSize,imageIndices).to(device)

            optimizer.zero_grad()

            outputs = model(images).to(device)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            losses.append(loss.item())

        model.eval()
        correct = []
        with torch.no_grad():
            for j in tqdm(testBatches):
                images = stackImages(j,batchSize,imageIndices).to(device)
                labels = stackLabels(j,batchSize,imageIndices).to(device)

                outputs = model(images).to(device)
                top = torch.topk(outputs,k,dim=1)
                predictions = top.indices.to(device)

                for p,l in zip(predictions,labels):
                    correct.append(int(l in p))

        print(f'Loss: {np.array(losses).mean()}')
        print(f'Test accuracy: {np.array(correct).mean()*100}%')
        testAccuracies.append(np.array(correct).mean())
        epoch += 1

        if len(testAccuracies)>=4:
            if testAccuracies[-1]<testAccuracies[-2] and testAccuracies[-1]<testAccuracies[-3] and testAccuracies[-1]<testAccuracies[-4]:
                print('Test accuracy has converged. Previous model (of higher accuracy) saved.')
                keepGoing = False
                break
            else:
                torch.save(model.state_dict(), f'models/plantvision-model-{feature}.pt')
        
    del model